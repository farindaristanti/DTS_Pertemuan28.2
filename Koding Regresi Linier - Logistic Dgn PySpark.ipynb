{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#(Ref. Big Data Community dan https://spark.apache.org/docs/latest/mllib-linear-methods.html) MK Analisis Big Data Filkom UB (Imam Cholissodin | imamcs@ub.ac.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresi Linier (dengan data Simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [10.754139100117724]\n",
      "Intercept: 829.9665547533602\n",
      "numIterations: 3\n",
      "objectiveHistory: [0.5000000000000142, 0.41689712560766806, 0.14247223398876466]\n",
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|-20.886257858187037|\n",
      "| -30.69006615971739|\n",
      "| -82.47731806077672|\n",
      "|  14.30993384028261|\n",
      "| 103.83476844098914|\n",
      "| -24.65695335877558|\n",
      "|  -96.7231789606592|\n",
      "| 24.030960139458557|\n",
      "|  56.76854283910552|\n",
      "|  56.48956913828124|\n",
      "+-------------------+\n",
      "\n",
      "RMSE: 60.101266\n",
      "r2: 0.719013\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format(\"libsvm\")\\\n",
    "    .load(\"data/my/simplereg.txt\")\n",
    "\n",
    "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for linear regression\n",
    "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
    "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
    "\n",
    "# Summarize the model over the training set and print out some metrics\n",
    "trainingSummary = lrModel.summary\n",
    "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
    "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
    "trainingSummary.residuals.show()\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- y: integer (nullable = true)\n",
      " |-- x: integer (nullable = true)\n",
      "\n",
      "+--------+----+\n",
      "|features|   y|\n",
      "+--------+----+\n",
      "|  [41.0]|1250|\n",
      "|  [54.0]|1380|\n",
      "|  [63.0]|1425|\n",
      "|  [54.0]|1425|\n",
      "|  [48.0]|1450|\n",
      "|  [46.0]|1300|\n",
      "|  [62.0]|1400|\n",
      "|  [61.0]|1510|\n",
      "|  [64.0]|1575|\n",
      "|  [71.0]|1650|\n",
      "+--------+----+\n",
      "\n",
      "Coefficients: [10.754139100117724]\n",
      "Intercept: 829.9665547533602\n",
      "+-------+----------------+\n",
      "|summary|               y|\n",
      "+-------+----------------+\n",
      "|  count|              10|\n",
      "|   mean|          1436.5|\n",
      "| stddev|119.514062398996|\n",
      "|    min|            1250|\n",
      "|    max|            1650|\n",
      "+-------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "house_df = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('data/my/simplereg.csv')\n",
    "house_df.take(1)\n",
    "house_df.cache()\n",
    "house_df.printSchema()\n",
    "\n",
    "house_df.describe().toPandas().transpose()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "numeric_features = [t[0] for t in house_df.dtypes if t[1] == 'int' or t[1] == 'double']\n",
    "sampled_data = house_df.select(numeric_features).sample(False, 0.8).toPandas()\n",
    "axs = pd.scatter_matrix(sampled_data, figsize=(10, 10))\n",
    "n = len(sampled_data.columns)\n",
    "for i in range(n):\n",
    "    v = axs[i, 0]\n",
    "    v.yaxis.label.set_rotation(0)\n",
    "    v.yaxis.label.set_ha('right')\n",
    "    v.set_yticks(())\n",
    "    h = axs[n-1, i]\n",
    "    h.xaxis.label.set_rotation(90)\n",
    "    h.set_xticks(())\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vectorAssembler = VectorAssembler(inputCols = ['x'], outputCol = 'features')\n",
    "vhouse_df = vectorAssembler.transform(house_df)\n",
    "vhouse_df = vhouse_df.select(['features', 'y'])\n",
    "vhouse_df.show(10)\n",
    "\n",
    "#splits = vhouse_df.randomSplit([0.7, 0.3])\n",
    "splits = vhouse_df.randomSplit([1.0, 0.0])\n",
    "train_df = splits[0]\n",
    "#test_df = splits[1]\n",
    "test_df = splits[0]\n",
    "    \n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol = 'features', labelCol='y', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lr_model = lr.fit(train_df)\n",
    "print(\"Coefficients: \" + str(lr_model.coefficients))\n",
    "print(\"Intercept: \" + str(lr_model.intercept))\n",
    "\n",
    "train_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresi Linier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 7.4510328101026015\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD, LinearRegressionModel\n",
    "\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.replace(',', ' ').split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])\n",
    "\n",
    "data = sc.textFile(\"data/mllib/ridge-data/lpsa.data\")\n",
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "# Build the model\n",
    "model = LinearRegressionWithSGD.train(parsedData, iterations=100, step=0.00000001)\n",
    "\n",
    "# Evaluate the model on training data\n",
    "valuesAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "MSE = valuesAndPreds \\\n",
    "    .map(lambda vp: (vp[0] - vp[1])**2) \\\n",
    "    .reduce(lambda x, y: x + y) / valuesAndPreds.count()\n",
    "print(\"Mean Squared Error = \" + str(MSE))\n",
    "\n",
    "# Save and load model\n",
    "#model.save(sc, \"target/tmp/pythonLinearRegressionWithSGDModel\")\n",
    "#sameModel = LinearRegressionModel.load(sc, \"target/tmp/pythonLinearRegressionWithSGDModel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regressionî§‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error = 0.36645962732919257\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS, LogisticRegressionModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Load and parse the data\n",
    "def parsePoint(line):\n",
    "    values = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(values[0], values[1:])\n",
    "\n",
    "data = sc.textFile(\"data/mllib/sample_svm_data.txt\")\n",
    "parsedData = data.map(parsePoint)\n",
    "\n",
    "# Build the model\n",
    "model = LogisticRegressionWithLBFGS.train(parsedData)\n",
    "\n",
    "# Evaluating the model on training data\n",
    "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
    "trainErr = labelsAndPreds.filter(lambda lp: lp[0] != lp[1]).count() / float(parsedData.count())\n",
    "print(\"Training Error = \" + str(trainErr))\n",
    "\n",
    "# Save and load model\n",
    "model.save(sc, \"target/tmp/pythonLogisticRegressionWithLBFGSModel\")\n",
    "sameModel = LogisticRegressionModel.load(sc,\n",
    "                                         \"target/tmp/pythonLogisticRegressionWithLBFGSModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: Logistic Regression (lainnya)\n",
    "\n",
    "mencoba custom menggunakan DataFrames (vs. Logistic Reg dari Spark Mllib yang menggunakan RDDs)\n",
    "\n",
    "### Konsep:\n",
    "\n",
    "DataFrame: API Machine Learning ini menggunakan DataFrame dari Spark SQL sebagai dataset Machine Learning, yang dapat menampung berbagai jenis data. Misalnya, DataFrame dapat memiliki kolom berbeda dalam menyimpan teks, vektor fitur, label aktual, dan prediksi.\n",
    "\n",
    "Transformer: algoritma yang dapat mengubah satu DataFrame menjadi DataFrame lain. Misalnya, model Machine Learning adalah suatu alat Transformer atau transformasi yang dapat mengubah DataFrame yang berisi fitur-fitur (input) menjadi DataFrame dalam bentuk prediksi (output).\n",
    "\n",
    "Estimator: algoritma yang dapat masuk dalam bentuk DataFrame untuk menghasilkan Transformer. Misalnya, algoritme pembelajaran adalah suatu Estimator yang melakukan proses pelatihan dari DataFrame untuk menghasilkan model yang siap digunakan untuk proses pengujian.\n",
    "\n",
    "Pipeline: Sebuah Pipeline memadukan beberapa Transformer dan Estimator bersama-sama untuk menentukan alur kerja dari suatu Machine Learning.\n",
    "\n",
    "(sumber: spark.apache.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
